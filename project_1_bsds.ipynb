{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3647abcb",
   "metadata": {},
   "source": [
    "## Project 1 \n",
    "\n",
    "The goal of the first project is to do some wrangling, EDA, and visualization, and generate sequences of values. We will focus on:\n",
    "\n",
    "- CDC National Health and Nutritional Examination Survey (NHANES, 1999-2000): https://wwwn.cdc.gov/nchs/nhanes/continuousnhanes/default.aspx?BeginYear=1999\n",
    "- CDC Linked Mortality File (LMF, 1999-2000): https://www.cdc.gov/nchs/data-linkage/mortality-public.htm\n",
    "\n",
    "NHANES is a rich panel dataset on health and behavior, collected bi-yearly from around 1999 to now. We will focus on the 1999 wave, because that has the largest follow-up window, providing us with the richest mortality data. The mortality data is provided by the CDC Linked Mortality File. \n",
    "\n",
    "The purpose of the project is to use $k$-NN to predict who dies (hard or soft classification) and how long they live (regression).\n",
    "\n",
    "### Part 1: Wrangling and EDA (40/100 pts)\n",
    "\n",
    "First, go to the NHANES and LMF web sites and familiarize yourself with the data sources. Download codebooks. Think about what resources are available. The CDC Linked Mortality File is somewhat of a pain to work with, so I have pre-cleaned it for you. It is available at httts://github.com/ds4e/undergraduate_ml_assignments in the data folder, as `lmf_parsed.cav`. From the CDC LMF web page, get the SAS program to load the data; it is the real codebook.\n",
    "\n",
    "Second, download the demographic data for the 1999--2000 wave from the NHANES page. You can use the following code chunk to merge the LMF and DEMO data:\n",
    "\n",
    "``` python\n",
    "import pandas as pd\n",
    "mdf = pd.read_csv('linked_mortality_file_1999_2000.csv') # Load mortality file\n",
    "print( mdf.head() )\n",
    "gdf = pd.read_sas(\"DEMO.xpt\", format=\"xport\") # Load demographics file\n",
    "print( gdf.head() )\n",
    "df = gdf.merge(mdf, on=\"SEQN\", how=\"inner\") # Merge mortality and demographics on SEQN variable\n",
    "```\n",
    "\n",
    "Third, the variables `ELIGSTAT`, `MORTSTAT`, `PERMTH_INT`, and `RIDAGEEX` are particularly important. Look them up in the documentation and clearly describe them. (5/100 pts.)\n",
    "\n",
    "Second, the goal of the project is to use whatever demographic, behavioral, and health data you like to predict mortality (`MORTSTAT`) and life expectancy (`PERMTH_INT`). Go to the NHANES 1999--2000 web page and select your data and download it. Clearly explain your rationale for selecting these data. Use `.merge` to combine your data into one complete dataframe. Document missing values. (5/100 pts)\n",
    "\n",
    "Third, do basic EDA and visualization of the key variables. Are any important variables skewed? Are there outliers? How correlated are pairs of variables? Do pairs of categorical variables exhibit interesting patterns in contingency tables? Provide a clear discussion and examination of the data and the variables you are interested in using. (20/100 pts)\n",
    "\n",
    "\n",
    "### Part 2: $k$-NN classification/regression, write-up (50/100 pts)\n",
    "\n",
    "Submit a notebook that clearly addresses the following, using code and markdown chunks:\n",
    "\n",
    "1. Describe the data, particularly what an observation is and whether there are any missing data that might impact your analysis. Who collected the data and why? What known limitations are there to analysis? (10/100 pts)\n",
    "2. Describe the variables you selected to predict mortality and life expectancy, and the rationale behind them. Analyze your variables using describe tables, kernel densities, scatter plots, and conditional kernel densities. Are there any patterns of interest to notice? (10/100 pts)\n",
    "3. Using your variables to predict mortality using a $k$-Nearest Neighbor Classifier. Analyze its performance and explain clearly how you select $k$. (10/100 pts)\n",
    "4. Using your variables to predict life expectancy using a $k$-Nearest Neighbor Regressor. Analyze its performance and explain clearly how you select $k$. (10/100 pts)\n",
    "5. Describe how your model could be used for health interventions based on patient characteristics. Are there any limitations or risks to consider? (10/100 pts)\n",
    "\n",
    "## Submission (10/100 pts)\n",
    "\n",
    "Submit your work in a well-organized GitHub repo, where the code is appropriately commented and all members of the group have made significant contributions to the commit history. (10/100 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96796132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SEQN  ELIGSTAT  MORTSTAT  UCOD_LEADING  DIABETES  HYPERTEN  PERMTH_INT  \\\n",
      "0     1         2       NaN           NaN       NaN       NaN         NaN   \n",
      "1     2         1       1.0           6.0       0.0       0.0       177.0   \n",
      "2     3         2       NaN           NaN       NaN       NaN         NaN   \n",
      "3     4         2       NaN           NaN       NaN       NaN         NaN   \n",
      "4     5         1       0.0           NaN       NaN       NaN       244.0   \n",
      "\n",
      "   PERMTH_EXM  \n",
      "0         NaN  \n",
      "1       177.0  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4       244.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\l'\n",
      "C:\\Users\\ayanr\\AppData\\Local\\Temp\\ipykernel_16100\\3071396666.py:2: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  mdf = pd.read_csv('data\\linked_mortality_file_1999_2000.csv') # Load mortality file\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DEMO.xpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m mdf = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mlinked_mortality_file_1999_2000.csv\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Load mortality file\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m( mdf.head() )\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m gdf = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_sas\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDEMO.xpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mxport\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Load demographics file\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m( gdf.head() )\n\u001b[32m      6\u001b[39m df = gdf.merge(mdf, on=\u001b[33m\"\u001b[39m\u001b[33mSEQN\u001b[39m\u001b[33m\"\u001b[39m, how=\u001b[33m\"\u001b[39m\u001b[33minner\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# Merge mortality and demographics on SEQN variable\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\sas\\sasreader.py:173\u001b[39m, in \u001b[36mread_sas\u001b[39m\u001b[34m(filepath_or_buffer, format, index, encoding, chunksize, iterator, compression)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m.lower() == \u001b[33m\"\u001b[39m\u001b[33mxport\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msas_xport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XportReader\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     reader = \u001b[43mXportReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m.lower() == \u001b[33m\"\u001b[39m\u001b[33msas7bdat\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msas7bdat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAS7BDATReader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\sas\\sas_xport.py:252\u001b[39m, in \u001b[36mXportReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, index, encoding, chunksize, compression)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28mself\u001b[39m._index = index\n\u001b[32m    250\u001b[39m \u001b[38;5;28mself\u001b[39m._chunksize = chunksize\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[38;5;28mself\u001b[39m.filepath_or_buffer = \u001b[38;5;28mself\u001b[39m.handles.handle\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\pandas\\io\\common.py:935\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    926\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    927\u001b[39m             handle,\n\u001b[32m    928\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    931\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    932\u001b[39m         )\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m935\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    936\u001b[39m     handles.append(handle)\n\u001b[32m    938\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'DEMO.xpt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mdf = pd.read_csv('data\\linked_mortality_file_1999_2000.csv') # Load mortality file\n",
    "print( mdf.head() )\n",
    "gdf = pd.read_sas(\"data\\DEMO.xpt\", format=\"xport\") # Load demographics file\n",
    "print( gdf.head() )\n",
    "df = gdf.merge(mdf, on=\"SEQN\", how=\"inner\") # Merge mortality and demographics on SEQN variable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
